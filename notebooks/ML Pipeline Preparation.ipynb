{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/victorl/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/victorl/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/victorl/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(['punkt', 'wordnet', 'stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df = pd.read_sql_table('labeled_messages', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['message']\n",
    "Y = df.drop(['message', 'genre', 'id', 'original'], axis=1)\n",
    "category_names = Y.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "remove_punc_table = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # normalize case and remove punctuation\n",
    "    text = text.translate(remove_punc_table).lower()\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # lemmatize and remove stop words\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "- You'll find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier(n_estimators=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "                    ('tfidf', TfidfVectorizer(tokenizer=tokenize)),\n",
    "                    ('forest', MultiOutputClassifier(forest_clf))\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,..._score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=None))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.34      0.14      0.20      2096\n",
      "         1.0       0.77      0.91      0.83      6497\n",
      "\n",
      "   micro avg       0.72      0.72      0.72      8593\n",
      "   macro avg       0.55      0.53      0.52      8593\n",
      "weighted avg       0.66      0.72      0.68      8593\n",
      "\n",
      "request\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.97      0.90      7117\n",
      "         1.0       0.38      0.08      0.13      1476\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      8593\n",
      "   macro avg       0.61      0.53      0.52      8593\n",
      "weighted avg       0.76      0.82      0.77      8593\n",
      "\n",
      "offer\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00      8547\n",
      "         1.0       0.00      0.00      0.00        46\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      8593\n",
      "   macro avg       0.50      0.50      0.50      8593\n",
      "weighted avg       0.99      0.99      0.99      8593\n",
      "\n",
      "aid_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.80      0.68      5105\n",
      "         1.0       0.42      0.22      0.29      3488\n",
      "\n",
      "   micro avg       0.56      0.56      0.56      8593\n",
      "   macro avg       0.51      0.51      0.48      8593\n",
      "weighted avg       0.53      0.56      0.52      8593\n",
      "\n",
      "medical_help\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.99      0.96      7924\n",
      "         1.0       0.06      0.00      0.01       669\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      8593\n",
      "   macro avg       0.49      0.50      0.48      8593\n",
      "weighted avg       0.85      0.92      0.88      8593\n",
      "\n",
      "medical_products\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.97      8161\n",
      "         1.0       0.12      0.01      0.02       432\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      8593\n",
      "   macro avg       0.54      0.50      0.49      8593\n",
      "weighted avg       0.91      0.95      0.92      8593\n",
      "\n",
      "search_and_rescue\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.99      8348\n",
      "         1.0       0.00      0.00      0.00       245\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      8593\n",
      "   macro avg       0.49      0.50      0.49      8593\n",
      "weighted avg       0.94      0.97      0.96      8593\n",
      "\n",
      "security\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99      8426\n",
      "         1.0       0.00      0.00      0.00       167\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      8593\n",
      "   macro avg       0.49      0.50      0.50      8593\n",
      "weighted avg       0.96      0.98      0.97      8593\n",
      "\n",
      "military\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.98      8328\n",
      "         1.0       0.17      0.00      0.01       265\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      8593\n",
      "   macro avg       0.57      0.50      0.50      8593\n",
      "weighted avg       0.94      0.97      0.95      8593\n",
      "\n",
      "child_alone\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      8593\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      8593\n",
      "   macro avg       1.00      1.00      1.00      8593\n",
      "weighted avg       1.00      1.00      1.00      8593\n",
      "\n",
      "water\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      1.00      0.96      8038\n",
      "         1.0       0.06      0.00      0.01       555\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      8593\n",
      "   macro avg       0.50      0.50      0.49      8593\n",
      "weighted avg       0.88      0.93      0.90      8593\n",
      "\n",
      "food\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.99      0.94      7623\n",
      "         1.0       0.22      0.02      0.03       970\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      8593\n",
      "   macro avg       0.55      0.50      0.48      8593\n",
      "weighted avg       0.81      0.88      0.83      8593\n",
      "\n",
      "shelter\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.99      0.95      7833\n",
      "         1.0       0.15      0.01      0.02       760\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      8593\n",
      "   macro avg       0.53      0.50      0.49      8593\n",
      "weighted avg       0.84      0.91      0.87      8593\n",
      "\n",
      "clothing\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99      8453\n",
      "         1.0       0.00      0.00      0.00       140\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      8593\n",
      "   macro avg       0.49      0.50      0.50      8593\n",
      "weighted avg       0.97      0.98      0.98      8593\n",
      "\n",
      "money\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99      8388\n",
      "         1.0       0.00      0.00      0.00       205\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      8593\n",
      "   macro avg       0.49      0.50      0.49      8593\n",
      "weighted avg       0.95      0.98      0.96      8593\n",
      "\n",
      "missing_people\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99      8489\n",
      "         1.0       0.00      0.00      0.00       104\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      8593\n",
      "   macro avg       0.49      0.50      0.50      8593\n",
      "weighted avg       0.98      0.99      0.98      8593\n",
      "\n",
      "refugees\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.98      8318\n",
      "         1.0       0.00      0.00      0.00       275\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      8593\n",
      "   macro avg       0.48      0.50      0.49      8593\n",
      "weighted avg       0.94      0.97      0.95      8593\n",
      "\n",
      "death\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victorl/code/udacity/DSND_Term2/disaster_pipeline/venv/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      1.00      0.98      8231\n",
      "         1.0       0.00      0.00      0.00       362\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      8593\n",
      "   macro avg       0.48      0.50      0.49      8593\n",
      "weighted avg       0.92      0.96      0.94      8593\n",
      "\n",
      "other_aid\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.98      0.92      7497\n",
      "         1.0       0.13      0.02      0.03      1096\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      8593\n",
      "   macro avg       0.50      0.50      0.48      8593\n",
      "weighted avg       0.78      0.86      0.81      8593\n",
      "\n",
      "infrastructure_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      1.00      0.97      8066\n",
      "         1.0       0.05      0.00      0.01       527\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      8593\n",
      "   macro avg       0.50      0.50      0.49      8593\n",
      "weighted avg       0.88      0.93      0.91      8593\n",
      "\n",
      "transport\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      1.00      0.98      8232\n",
      "         1.0       0.06      0.00      0.01       361\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      8593\n",
      "   macro avg       0.51      0.50      0.49      8593\n",
      "weighted avg       0.92      0.96      0.94      8593\n",
      "\n",
      "buildings\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.97      8153\n",
      "         1.0       0.00      0.00      0.00       440\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      8593\n",
      "   macro avg       0.47      0.50      0.49      8593\n",
      "weighted avg       0.90      0.95      0.92      8593\n",
      "\n",
      "electricity\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99      8417\n",
      "         1.0       0.00      0.00      0.00       176\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      8593\n",
      "   macro avg       0.49      0.50      0.49      8593\n",
      "weighted avg       0.96      0.98      0.97      8593\n",
      "\n",
      "tools\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00      8546\n",
      "         1.0       0.00      0.00      0.00        47\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      8593\n",
      "   macro avg       0.50      0.50      0.50      8593\n",
      "weighted avg       0.99      0.99      0.99      8593\n",
      "\n",
      "hospitals\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99      8501\n",
      "         1.0       0.00      0.00      0.00        92\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      8593\n",
      "   macro avg       0.49      0.50      0.50      8593\n",
      "weighted avg       0.98      0.99      0.98      8593\n",
      "\n",
      "shops\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      8553\n",
      "         1.0       0.00      0.00      0.00        40\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      8593\n",
      "   macro avg       0.50      0.50      0.50      8593\n",
      "weighted avg       0.99      1.00      0.99      8593\n",
      "\n",
      "aid_centers\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99      8500\n",
      "         1.0       0.25      0.01      0.02        93\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      8593\n",
      "   macro avg       0.62      0.51      0.51      8593\n",
      "weighted avg       0.98      0.99      0.98      8593\n",
      "\n",
      "other_infrastructure\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      1.00      0.98      8242\n",
      "         1.0       0.09      0.01      0.01       351\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      8593\n",
      "   macro avg       0.52      0.50      0.49      8593\n",
      "weighted avg       0.92      0.96      0.94      8593\n",
      "\n",
      "weather_related\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.94      0.83      6190\n",
      "         1.0       0.51      0.16      0.24      2403\n",
      "\n",
      "   micro avg       0.72      0.72      0.72      8593\n",
      "   macro avg       0.63      0.55      0.53      8593\n",
      "weighted avg       0.68      0.72      0.66      8593\n",
      "\n",
      "floods\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.99      0.96      7903\n",
      "         1.0       0.16      0.01      0.02       690\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      8593\n",
      "   macro avg       0.54      0.50      0.49      8593\n",
      "weighted avg       0.86      0.92      0.88      8593\n",
      "\n",
      "storm\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.99      0.95      7777\n",
      "         1.0       0.35      0.03      0.05       816\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      8593\n",
      "   macro avg       0.63      0.51      0.50      8593\n",
      "weighted avg       0.85      0.90      0.86      8593\n",
      "\n",
      "fire\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99      8499\n",
      "         1.0       0.00      0.00      0.00        94\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      8593\n",
      "   macro avg       0.49      0.50      0.50      8593\n",
      "weighted avg       0.98      0.99      0.98      8593\n",
      "\n",
      "earthquake\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.99      0.95      7770\n",
      "         1.0       0.62      0.16      0.26       823\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      8593\n",
      "   macro avg       0.77      0.58      0.61      8593\n",
      "weighted avg       0.89      0.91      0.89      8593\n",
      "\n",
      "cold\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99      8414\n",
      "         1.0       0.00      0.00      0.00       179\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      8593\n",
      "   macro avg       0.49      0.50      0.49      8593\n",
      "weighted avg       0.96      0.98      0.97      8593\n",
      "\n",
      "other_weather\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.97      8139\n",
      "         1.0       0.06      0.00      0.00       454\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      8593\n",
      "   macro avg       0.50      0.50      0.49      8593\n",
      "weighted avg       0.90      0.95      0.92      8593\n",
      "\n",
      "direct_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.97      0.88      6910\n",
      "         1.0       0.37      0.07      0.11      1683\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      8593\n",
      "   macro avg       0.59      0.52      0.50      8593\n",
      "weighted avg       0.73      0.80      0.73      8593\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(Y_test):\n",
    "    print(col)\n",
    "    print(classification_report(Y_test[col], Y_pred[:, i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'tfidf__ngram_range': ((1, 1), (1, 2)),\n",
    "    'tfidf__max_df': (0.8, 1.0),\n",
    "    'tfidf__max_features': (None, 10000),\n",
    "    'forest__estimator__n_estimators': [50, 100],\n",
    "    'forest__estimator__min_samples_split': [2, 4]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, parameters, cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-459-843b9446bae3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/udacity/DSND_Term2/disaster_pipeline/venv/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/udacity/DSND_Term2/disaster_pipeline/venv/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/udacity/DSND_Term2/disaster_pipeline/venv/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/udacity/DSND_Term2/disaster_pipeline/venv/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/udacity/DSND_Term2/disaster_pipeline/venv/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/udacity/DSND_Term2/disaster_pipeline/venv/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, Y_test):\n",
    "    Y_pred = model.predict(X_test)\n",
    "    print(classification_report(Y_test, Y_pred, target_names=category_names))\n",
    "#     print('Accuracy: ', accuracy_score(Y_test, Y_pred))\n",
    "#     print('Precision: ', precision_score(Y_test, Y_pred, average='weighted'))\n",
    "#     print('Recall: ', recall_score(Y_test, Y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.14802746421505877\n",
      "Precision:  0.3900151724208853\n",
      "Recall:  0.27685873949890827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victorl/code/udacity/DSND_Term2/disaster_pipeline/venv/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', accuracy_score(Y_test, Y_pred))\n",
    "print('Precision: ', precision_score(Y_test, Y_pred, average='weighted'))\n",
    "print('Recall: ', recall_score(Y_test, Y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_null(series):\n",
    "\n",
    "    def is_null_base(s):\n",
    "        res = 0 if s is None else 1\n",
    "        return res\n",
    "    \n",
    "    final = pd.DataFrame(series.applymap(is_null_base))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['genre']\n",
    "categorical_transform = OneHotEncoder()\n",
    "\n",
    "original = ['original']\n",
    "original_transform = FunctionTransformer(is_null, validate=False)\n",
    "\n",
    "text = 'message'\n",
    "text_transform = Pipeline([('tfidf', TfidfVectorizer(tokenizer=tokenize)),\n",
    "                           ('best', TruncatedSVD(n_components=50))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(transformers=[\n",
    "                                ('cat', categorical_transform, categorical_features),\n",
    "                                ('orig', original_transform, original),\n",
    "                                ('tfidf', text_transform, 'message')\n",
    "                                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         2.92220547e-02, -4.33138260e-02, -2.60150053e-02],\n",
       "       [ 1.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         3.10686205e-02, -1.99762261e-02,  1.34438408e-02],\n",
       "       [ 1.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -1.33582327e-02, -3.16578759e-02,  4.13029577e-02],\n",
       "       ...,\n",
       "       [ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00, ...,\n",
       "         3.37286416e-02,  8.27223301e-03, -5.65770821e-03],\n",
       "       [ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00, ...,\n",
       "         6.45141232e-04, -7.53957391e-03,  8.45396210e-03],\n",
       "       [ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00, ...,\n",
       "        -5.62647930e-03,  2.18488046e-02, -3.19588149e-02]])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.fit_transform(X_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After working on this for a bit I decided to check how the code for the webserver would work and noted that there was a section that took new user input - this won't have `original` or `genre` so this entire column transformer is pretty moot. From here I'm going to go back to just using the tfidf + some other text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = forest_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_2 = Pipeline([\n",
    "                    ('tfidf', TfidfVectorizer(tokenizer=tokenize)),\n",
    "                    ('best', TruncatedSVD(n_components=100)),\n",
    "                    ('clf', MultiOutputClassifier(clf))\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,..._score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=None))])"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test to make sure this works ok\n",
    "pipeline_2.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function tokenize at 0x1027411e0>, use_idf=True,\n",
      "        vocabulary=None)), ('best', TruncatedSVD(algorithm='randomized', n_components=100, n_iter=5,\n",
      "       random_state=None, tol=0.0)), ('clf', MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False),\n",
      "           n_jobs=None))]\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_2.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.77      0.87      0.82      6497\n",
      "               request       0.38      0.12      0.18      1476\n",
      "                 offer       0.00      0.00      0.00        46\n",
      "           aid_related       0.43      0.25      0.31      3488\n",
      "          medical_help       0.19      0.01      0.01       669\n",
      "      medical_products       0.11      0.00      0.00       432\n",
      "     search_and_rescue       0.25      0.00      0.01       245\n",
      "              security       0.00      0.00      0.00       167\n",
      "              military       0.00      0.00      0.00       265\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.07      0.00      0.00       555\n",
      "                  food       0.19      0.02      0.03       970\n",
      "               shelter       0.10      0.01      0.01       760\n",
      "              clothing       0.00      0.00      0.00       140\n",
      "                 money       0.00      0.00      0.00       205\n",
      "        missing_people       0.00      0.00      0.00       104\n",
      "              refugees       0.00      0.00      0.00       275\n",
      "                 death       0.00      0.00      0.00       362\n",
      "             other_aid       0.11      0.01      0.02      1096\n",
      "infrastructure_related       0.15      0.00      0.01       527\n",
      "             transport       0.00      0.00      0.00       361\n",
      "             buildings       0.08      0.00      0.00       440\n",
      "           electricity       0.00      0.00      0.00       176\n",
      "                 tools       0.00      0.00      0.00        47\n",
      "             hospitals       0.00      0.00      0.00        92\n",
      "                 shops       0.00      0.00      0.00        40\n",
      "           aid_centers       0.00      0.00      0.00        93\n",
      "  other_infrastructure       0.25      0.00      0.01       351\n",
      "       weather_related       0.45      0.16      0.24      2403\n",
      "                floods       0.12      0.00      0.01       690\n",
      "                 storm       0.49      0.05      0.08       816\n",
      "                  fire       0.00      0.00      0.00        94\n",
      "            earthquake       0.65      0.14      0.23       823\n",
      "                  cold       0.00      0.00      0.00       179\n",
      "         other_weather       0.00      0.00      0.00       454\n",
      "         direct_report       0.32      0.09      0.14      1683\n",
      "\n",
      "             micro avg       0.63      0.27      0.38     27021\n",
      "             macro avg       0.14      0.05      0.06     27021\n",
      "          weighted avg       0.39      0.27      0.29     27021\n",
      "           samples avg       0.61      0.32      0.36     27021\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victorl/code/udacity/DSND_Term2/disaster_pipeline/venv/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/victorl/code/udacity/DSND_Term2/disaster_pipeline/venv/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/victorl/code/udacity/DSND_Term2/disaster_pipeline/venv/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/victorl/code/udacity/DSND_Term2/disaster_pipeline/venv/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(pipeline_2, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'tfidf__ngram_range': ((1, 1), (1, 2)),\n",
    "    'tfidf__max_df': (0.8, 1.0),\n",
    "    'tfidf__max_features': (None, 10000),\n",
    "    'best__n_components': (10, 50, 100),\n",
    "    'clf__estimator__n_estimators': [50, 100],\n",
    "    'clf__estimator__min_samples_split': [2, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_2 = GridSearchCV(pipeline_2, param_grid, cv=3, verbose=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 15.7min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed: 19.3min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed: 21.3min\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed: 25.2min\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed: 31.9min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed: 38.7min\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed: 42.7min\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed: 60.2min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 71.5min\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed: 80.1min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 95.6min\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed: 114.0min\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed: 141.5min\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed: 162.7min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed: 182.2min\n",
      "[Parallel(n_jobs=-1)]: Done 288 out of 288 | elapsed: 204.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,..._score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=None))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'tfidf__ngram_range': ((1, 1), (1, 2)), 'tfidf__max_df': (0.8, 1.0), 'tfidf__max_features': (None, 10000), 'best__n_components': (10, 50, 100), 'clf__estimator__n_estimators': [50, 100], 'clf__estimator__min_samples_split': [2, 4]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=10)"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_2.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tfidf',\n",
      "  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function tokenize at 0x1027411e0>, use_idf=True,\n",
      "        vocabulary=None)),\n",
      " ('best',\n",
      "  TruncatedSVD(algorithm='randomized', n_components=100, n_iter=5,\n",
      "       random_state=None, tol=0.0)),\n",
      " ('clf',\n",
      "  MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False),\n",
      "           n_jobs=None))]\n"
     ]
    }
   ],
   "source": [
    "pprint(cv_2.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.76      0.98      0.86      6497\n",
      "               request       0.35      0.05      0.08      1476\n",
      "                 offer       0.00      0.00      0.00        46\n",
      "           aid_related       0.45      0.18      0.26      3488\n",
      "          medical_help       0.00      0.00      0.00       669\n",
      "      medical_products       0.00      0.00      0.00       432\n",
      "     search_and_rescue       0.00      0.00      0.00       245\n",
      "              security       0.00      0.00      0.00       167\n",
      "              military       0.00      0.00      0.00       265\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.00      0.00      0.00       555\n",
      "                  food       0.11      0.00      0.00       970\n",
      "               shelter       0.21      0.00      0.01       760\n",
      "              clothing       0.00      0.00      0.00       140\n",
      "                 money       0.00      0.00      0.00       205\n",
      "        missing_people       0.00      0.00      0.00       104\n",
      "              refugees       0.00      0.00      0.00       275\n",
      "                 death       0.00      0.00      0.00       362\n",
      "             other_aid       0.00      0.00      0.00      1096\n",
      "infrastructure_related       0.33      0.00      0.00       527\n",
      "             transport       0.00      0.00      0.00       361\n",
      "             buildings       0.00      0.00      0.00       440\n",
      "           electricity       0.00      0.00      0.00       176\n",
      "                 tools       0.00      0.00      0.00        47\n",
      "             hospitals       0.00      0.00      0.00        92\n",
      "                 shops       0.00      0.00      0.00        40\n",
      "           aid_centers       0.00      0.00      0.00        93\n",
      "  other_infrastructure       1.00      0.00      0.01       351\n",
      "       weather_related       0.60      0.15      0.24      2403\n",
      "                floods       1.00      0.00      0.00       690\n",
      "                 storm       0.46      0.03      0.06       816\n",
      "                  fire       0.00      0.00      0.00        94\n",
      "            earthquake       0.75      0.17      0.28       823\n",
      "                  cold       0.00      0.00      0.00       179\n",
      "         other_weather       0.00      0.00      0.00       454\n",
      "         direct_report       0.35      0.04      0.07      1683\n",
      "\n",
      "             micro avg       0.69      0.28      0.40     27021\n",
      "             macro avg       0.18      0.04      0.05     27021\n",
      "          weighted avg       0.43      0.28      0.28     27021\n",
      "           samples avg       0.70      0.35      0.40     27021\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victorl/code/udacity/DSND_Term2/disaster_pipeline/venv/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/victorl/code/udacity/DSND_Term2/disaster_pipeline/venv/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/victorl/code/udacity/DSND_Term2/disaster_pipeline/venv/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/victorl/code/udacity/DSND_Term2/disaster_pipeline/venv/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(cv_2.best_estimator_, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['disaster_model.pkl']"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(cv_2.best_estimator_, 'disaster_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train_classifier.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
